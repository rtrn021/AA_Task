{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaa78fa",
   "metadata": {},
   "source": [
    "# Imports and Method, Variables Declerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f1ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,concat\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import boto3\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "access_key = credentials.access_key\n",
    "secret_key = credentials.secret_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e66875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark():\n",
    "    \"\"\"\n",
    "    Creating spark to be able to read data from S3 bucket\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    spark = (\n",
    "        SparkSession\n",
    "            .builder\n",
    "            .master(\"local[*]\")\n",
    "            .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.2')\n",
    "            .config(\"fs.s3a.access.key\", access_key)\n",
    "            .config(\"fs.s3a.secret.key\", secret_key)\n",
    "            .config('spark.hadoop.fs.s3a.aws.credentials.provider',\n",
    "                    'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "            .config(\"fs.s3a.endpoint.key\", \"s3.amazonaws.com\")\n",
    "            .config(\"spark.executor.memory\", \"70g\")\n",
    "            .config(\"spark.driver.memory\", \"50g\")\n",
    "            .config(\"spark.memory.offHeap.enabled\", True)\n",
    "            .config(\"spark.memory.offHeap.size\", \"32g\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "\n",
    "\n",
    "def compare_validate_schema(df_ms, df):\n",
    "    \"\"\"\n",
    "    Validate schema by comparing mapping spec and Dataframe schema\n",
    "    :param df_ms: Pandas DataFrame, expected schema\n",
    "    :param df: Spark DataFrame\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    all_columns = set()\n",
    "    all_columns.update(df.columns)\n",
    "    all_columns.update(df_ms['column_name'])\n",
    "    all_columns = list(all_columns)\n",
    "\n",
    "    df_result = pd.DataFrame(columns=['column_name', 'ms', 's3'])\n",
    "\n",
    "    for i in range(len(all_columns)):\n",
    "        col_name = all_columns[i]\n",
    "        df_result = df_result.append({'column_name': col_name,\n",
    "                                      'ms': (df_ms['data_type'][df_ms['column_name'] == col_name]).values[0],\n",
    "                                      's3': str(df.schema[col_name].dataType)}, ignore_index=True)\n",
    "    df_diff = df_result[df_result.isna().any(axis=1) | (df_result['ms'] != df_result['s3'])]\n",
    "    print('========= The difference between MS and S3 =====')\n",
    "    print(df_diff)\n",
    "#     assert len(df_diff) == 0, f'The MS and Data Schema does not match!!!'\n",
    "\n",
    "\n",
    "def validate_pk(spark, df, pk_column_list):\n",
    "    \"\"\"\n",
    "    Checks PK of dataframe\n",
    "    :param spark: Spark\n",
    "    :param df: Dataframe\n",
    "    :param pk_column_list: PK column list of the Dataframe\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView('table')\n",
    "    pk_columns_str = ','.join(pk_column_list)\n",
    "    query = f'select count(*) as duplicated_row_count from table group by {pk_columns_str} having count(*) > 1'\n",
    "    # df.groupBy(*pk_column_list).count().where(col('count') > 1) >> same function different way\n",
    "    result = spark.sql(query)\n",
    "    duplicated_rows = result.groupby().sum().collect()[0][0]\n",
    "    result.show()\n",
    "    print(f'{duplicated_rows} duplicated rows found!!!')\n",
    "#     assert duplicated_rows == None, f'{duplicated_rows} duplication found!!!'\n",
    "\n",
    "\n",
    "def validate_not_null_columns(spark, df, not_null_column_list):\n",
    "    \"\"\"\n",
    "    Checks not null for columns\n",
    "    :param spark: Spark\n",
    "    :param df: DataFrame\n",
    "    :param not_null_column_list: list of not-null columns\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView('table')\n",
    "    query = \"select \"\n",
    "    nn_columns = []\n",
    "    for column in not_null_column_list:\n",
    "        nn_columns.append(f'sum(case when {column} is null then 1 else 0 end) as {column} ')\n",
    "    query = query + ','.join(nn_columns) + ' from table'\n",
    "    result = spark.sql(query)\n",
    "    sum_null_values = sum(result.collect()[0])\n",
    "    result.show()\n",
    "    print(f'{sum_null_values} null values found!!!')\n",
    "#     assert sum_null_values == 0, f'{sum_null_values} null values found!!!'\n",
    "\n",
    "def validate_column_has_supported_values(df, column, supported_values):\n",
    "    \"\"\"\n",
    "    Checks if column only has supported values\n",
    "    :param df: Spark dataframe\n",
    "    :param column: column_name\n",
    "    :param supported_values: supported values list\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    column_values_list = df.select(column).filter(f'{column} is not Null').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "    unexpected_values_list = list(set(column_values_list) - set(supported_values))\n",
    "    print(f'list of unexpected values from column: {unexpected_values_list}')\n",
    "#     assert len(unexpected_values_list) == 0, f'{column} has unexpected values: {unexpected_values_list}!!!'\n",
    "\n",
    "def validate_max_length_of_column(df, column, max_length):\n",
    "    \"\"\"\n",
    "    Validate the char length of a column's values\n",
    "    :param df: Dataframe\n",
    "    :param column: column name\n",
    "    :param max_length: expected max length\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    count_outlier = 0\n",
    "    for search_term in df.filter(f'{column} is not Null').select(column).distinct().rdd.map(lambda r: r[0]).collect():\n",
    "        if len(search_term) > max_length:\n",
    "            count_outlier += 1\n",
    "    print(f'{count_outlier} outlier found!!!')\n",
    "#     assert count_outlier == 0, f'{count_outlier} outliers found!!!'\n",
    "\n",
    "\n",
    "def validate_column_value_greater_than(df, column, greater_than):\n",
    "    \"\"\"\n",
    "    Counts outliers for column value be greater than a value\n",
    "    Validates there is no outliers\n",
    "    The method skip the check for null values as not-null is getting checked in another step\n",
    "    :param df: dataframe\n",
    "    :param column: column name of  the dataframe\n",
    "    :param greater_than: value expected be greater than\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    count_outlier = df.filter(f'{column} is not Null').where(\n",
    "        f'{column} = {greater_than} or {column} < {greater_than}').count()\n",
    "    print(f'{count_outlier} outlier found!!!')\n",
    "#     assert count_outlier == 0, f'{count_outlier} outliers found!!! column: {column}, greater_than: {greater_than}'\n",
    "\n",
    "\n",
    "def validate_str_date_format(date_text, date_format):\n",
    "    \"\"\"\n",
    "    validate if the date_text matches to the date_format\n",
    "    :param date_text: date to be validated\n",
    "    :param format: format to be compared\n",
    "    :return: True if format matches\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if date_text != datetime.strptime(date_text, date_format).strftime(date_format):\n",
    "            raise ValueError\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def validate_date_format_for_column(df, column, date_format):\n",
    "    \"\"\"\n",
    "    Validates date format\n",
    "    Skip check for null values as not-null is getting checked in another step\n",
    "    :param df: dataframe\n",
    "    :param column: column to be checked\n",
    "    :param date_format: string format of the date\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for dt in df.select(column).filter(f'{column} is not Null').distinct().rdd.map(lambda r: r[0]).collect():\n",
    "        assert validate_str_date_format(str(dt), date_format), f'date format {dt} doesnt match to {date_format}!!!'\n",
    "    print('All dates date format matches!!! Null values are skipped in that check, as that check done in previous steps!!!')\n",
    "\n",
    "\n",
    "def validate_min_value_of_column(df, column, min_value):\n",
    "    \"\"\"\n",
    "    Counts outliers for min value of a column\n",
    "    Validates there is no outliers\n",
    "    The method skip the check for null values as not-null is getting checked in another step\n",
    "    :param df: dataframe\n",
    "    :param column: column name of  the dataframe\n",
    "    :param min_value: min value to be expected\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    count_outlier = df.filter(f'{column} is not Null').where(\n",
    "        f'{column} < {min_value}').count()\n",
    "    print(f'{count_outlier} outlier found!!!')\n",
    "#     assert count_outlier == 0, f'{count_outlier} outliers found!!! column: {column}, min_value: {min_value}'\n",
    "\n",
    "\n",
    "def validate_max_value_of_column(df, column, max_value):\n",
    "    \"\"\"\n",
    "    Counts outliers for max value of a column\n",
    "    Validates there is no outliers\n",
    "    The method skip the check for null values as not-null is getting checked in another step\n",
    "    :param df: dataframe\n",
    "    :param column: column name of  the dataframe\n",
    "    :param max_value: max value to be expected\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    count_outlier = df.filter(f'{column} is not Null').where(\n",
    "        f'{column} > {max_value}').count()\n",
    "    print(f'{count_outlier} outlier found!!!')\n",
    "#     assert count_outlier == 0, f'{count_outlier} outliers found!!! column: {column}, max_value: {max_value}'\n",
    "\n",
    "\n",
    "def get_path_as_string():\n",
    "    \"\"\"\n",
    "    Gets the path of the file\n",
    "    :return: path\n",
    "    \"\"\"\n",
    "    p = Path(__file__)\n",
    "    plist = str(p).split('/e2e/')\n",
    "    return str(plist[0]) + '/'\n",
    "\n",
    "\n",
    "def add_path_to_base_path(path_to_be_added):\n",
    "    \"\"\"\n",
    "    adds path(param) to main path(../test/)\n",
    "    :param path_to_be_added: should start from e2e/\n",
    "    :return: new path with string format\n",
    "    \"\"\"\n",
    "    return get_path_as_string() + path_to_be_added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0711ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_bucket = 'adthena.data.qa.test'\n",
    "\n",
    "supported_values_for_columns = {\n",
    "    'device': ['desktop', 'mobile']\n",
    "}\n",
    "\n",
    "date_format = '%Y-%m-%d'\n",
    "\n",
    "sa_ms_loc = '/Users/rtrn/PycharmProjects/AA_Task/test/e2e/data/scrape_appearances_ms.xlsx'\n",
    "ca_ms_loc = '/Users/rtrn/PycharmProjects/AA_Task/test/e2e/data/competitor_appearances_ms.xlsx'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1204a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_appearances schema pandas dataframe\n",
    "sa_ms_df = pd.read_excel(sa_ms_loc)\n",
    "\n",
    "# competitor_appearances schema pandas dataframe\n",
    "ca_ms_df = pd.read_excel(ca_ms_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099e5ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>pk</th>\n",
       "      <th>not_null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>search_term</td>\n",
       "      <td>StringType</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>device</td>\n",
       "      <td>StringType</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>date</td>\n",
       "      <td>DateType</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scrape_count</td>\n",
       "      <td>IntegerType</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    column_name    data_type   pk not_null\n",
       "0   search_term   StringType    Y        Y\n",
       "1        device   StringType    Y        Y\n",
       "2          date     DateType    Y        Y\n",
       "3  scrape_count  IntegerType  NaN        Y"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what we expect from scrape_appearances\n",
    "sa_ms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7181ed18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>pk</th>\n",
       "      <th>not_null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>search_term</td>\n",
       "      <td>StringType</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>device</td>\n",
       "      <td>StringType</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>date</td>\n",
       "      <td>DateType</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domain</td>\n",
       "      <td>StringType</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sponsored_appearances</td>\n",
       "      <td>IntegerType</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>natural_appearances</td>\n",
       "      <td>IntegerType</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pla_appearances</td>\n",
       "      <td>IntegerType</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ctr</td>\n",
       "      <td>DoubleType</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             column_name    data_type   pk not_null\n",
       "0            search_term   StringType    Y        Y\n",
       "1                 device   StringType    Y        Y\n",
       "2                   date     DateType    Y        Y\n",
       "3                 domain   StringType    Y        Y\n",
       "4  sponsored_appearances  IntegerType  NaN        Y\n",
       "5    natural_appearances  IntegerType  NaN        Y\n",
       "6        pla_appearances  IntegerType  NaN        Y\n",
       "7                    ctr   DoubleType  NaN      NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what we expect from competitor_appearances\n",
    "ca_ms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d0475",
   "metadata": {},
   "source": [
    "# 1) Scrape Appearances Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ae3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.endpoint.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/rtrn/Library/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/rtrn/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/rtrn/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ad792f40-184d-43ab-81da-ea4c9f436a5e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 282ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ad792f40-184d-43ab-81da-ea4c9f436a5e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/12ms)\n",
      "22/05/26 20:00:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/26 20:00:05 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- search_term: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- device: string (nullable = true)\n",
      " |-- scrape_count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+------------+\n",
      "|         search_term|      date| device|scrape_count|\n",
      "+--------------------+----------+-------+------------+\n",
      "|$100 000 personal...|2022-05-13|desktop|           1|\n",
      "|$135 nike men's b...|2022-05-13|desktop|           1|\n",
      "|$199 michael star...|2022-05-13|desktop|           1|\n",
      "|          $2000 loan|2022-05-13|desktop|           3|\n",
      "|             $250.63|2022-05-13|desktop|           1|\n",
      "+--------------------+----------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# scrape_appearances schema PySpark dataframe\n",
    "sa_df = get_spark().read.parquet('s3a://adthena.data.qa.test/scrape_appearances/*.parquet')\n",
    "sa_df.printSchema()\n",
    "sa_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb34d999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7143663"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many records exist in the dataframe\n",
    "sa_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['search_term', 'device', 'date']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PK list\n",
    "pk_column_list = sa_ms_df[sa_ms_df['pk'] == 'Y']['column_name'].values.tolist()\n",
    "pk_column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f61d96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['search_term', 'device', 'date', 'scrape_count']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not null column list\n",
    "not_null_column_list = sa_ms_df[sa_ms_df['not_null'] == 'Y']['column_name'].values.tolist()\n",
    "not_null_column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c3374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= The difference between MS and S3 =====\n",
      "Empty DataFrame\n",
      "Columns: [column_name, ms, s3]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Validate schema and get the differences between the schema we expect and the actual schema\n",
    "compare_validate_schema(sa_ms_df,sa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2bdcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|duplicated_row_count|\n",
      "+--------------------+\n",
      "|                   2|\n",
      "+--------------------+\n",
      "\n",
      "2 duplicated rows found!!!\n"
     ]
    }
   ],
   "source": [
    "# PK check\n",
    "validate_pk(get_spark(),sa_df,pk_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d27fcb80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+------------+\n",
      "|search_term|device|date|scrape_count|\n",
      "+-----------+------+----+------------+\n",
      "|          0|     0|   2|           1|\n",
      "+-----------+------+----+------------+\n",
      "\n",
      "3 null values found!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Null check\n",
    "validate_not_null_columns(get_spark(), sa_df, not_null_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4bc1c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of unexpected values from column: ['tablet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking weather device column has only supported values\n",
    "supported_values = supported_values_for_columns['device']\n",
    "validate_column_has_supported_values(sa_df,'device',supported_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2545ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2 values exist as \"tablet\" in device column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:===================================================>     (9 + 1) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count how many unexpected records exist in device column, value as 'tablet'\n",
    "count_unexpected_tablet_values = sa_df.filter(sa_df.device == 'tablet').count()\n",
    "print(f'total {count_unexpected_tablet_values} values exist as \"tablet\" in device column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32184e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 outlier found!!!\n"
     ]
    }
   ],
   "source": [
    "# max char length of the search_term column\n",
    "validate_max_length_of_column(sa_df, 'search_term',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7436d7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 33:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 outlier found!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# scrape_count values greater than 0\n",
    "validate_column_value_greater_than(sa_df,'scrape_count',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "427855df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dates date format matches!!! Null values are skipped in that check, as that check done in previous steps!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:===================================================>     (9 + 1) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# date column format, Null values skipped as that is checked in previous steps\n",
    "validate_date_format_for_column(sa_df, 'date', date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ad7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269d5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f90af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bf536b9",
   "metadata": {},
   "source": [
    "# 2) Competitor Appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df457002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:======================================================> (39 + 1) / 40]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- search_term: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- device: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- sponsored_appearances: integer (nullable = true)\n",
      " |-- natural_appearances: integer (nullable = true)\n",
      " |-- pla_appearances: integer (nullable = true)\n",
      " |-- ctr: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+--------------------+---------------------+-------------------+---------------+-------------------+\n",
      "|         search_term|      date| device|              domain|sponsored_appearances|natural_appearances|pla_appearances|                ctr|\n",
      "+--------------------+----------+-------+--------------------+---------------------+-------------------+---------------+-------------------+\n",
      "|       samuel jarvis|2022-05-14|desktop|   jarvisarchives.ca|                    0|                  1|              0|               null|\n",
      "|san diego charger...|2022-05-14|desktop|    cheaptickets.com|                    0|                  3|              0|               null|\n",
      "|       sand blasting|2022-05-14|desktop|       wikipedia.org|                    0|                  3|              0|               null|\n",
      "|sandals for beach...|2022-05-14|desktop|          zappos.com|                    0|                  0|              1|0.05480628460645676|\n",
      "|santa barbara flo...|2022-05-14|desktop|santabarbaracompa...|                    0|                  3|              0|0.11958937240498406|\n",
      "+--------------------+----------+-------+--------------------+---------------------+-------------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# competitor_appearances PySpark dataframe\n",
    "ca_df = get_spark().read.parquet('s3a://adthena.data.qa.test/competitor_appearances/*.parquet')\n",
    "ca_df.printSchema()\n",
    "ca_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc95e679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92805679"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many records exist in the dataframe\n",
    "ca_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a001ba58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['search_term', 'device', 'date', 'domain']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PK list\n",
    "pk_column_list = ca_ms_df[ca_ms_df['pk'] == 'Y']['column_name'].values.tolist()\n",
    "pk_column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6288a7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['search_term',\n",
       " 'device',\n",
       " 'date',\n",
       " 'domain',\n",
       " 'sponsored_appearances',\n",
       " 'natural_appearances',\n",
       " 'pla_appearances']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not null column list\n",
    "not_null_column_list = ca_ms_df[ca_ms_df['not_null'] == 'Y']['column_name'].values.tolist()\n",
    "not_null_column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec2cdc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= The difference between MS and S3 =====\n",
      "Empty DataFrame\n",
      "Columns: [column_name, ms, s3]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Validate schema and get the differences between the schema we expect and the actual schema\n",
    "compare_validate_schema(ca_ms_df,ca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28b03343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=====================================================>  (71 + 4) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|duplicated_row_count|\n",
      "+--------------------+\n",
      "+--------------------+\n",
      "\n",
      "None duplicated rows found!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 59:=====================================================>  (72 + 3) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# PK check\n",
    "validate_pk(get_spark(),ca_df,pk_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39d58b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:=====================================================>  (22 + 1) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+------+---------------------+-------------------+---------------+\n",
      "|search_term|device|date|domain|sponsored_appearances|natural_appearances|pla_appearances|\n",
      "+-----------+------+----+------+---------------------+-------------------+---------------+\n",
      "|          0|     0|   0|     1|                    0|                  0|              0|\n",
      "+-----------+------+----+------+---------------------+-------------------+---------------+\n",
      "\n",
      "1 null values found!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Null check\n",
    "validate_not_null_columns(get_spark(), ca_df, not_null_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bf0e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:===================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of unexpected values from column: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# device column has only supported values\n",
    "supported_values = supported_values_for_columns['device']\n",
    "validate_column_has_supported_values(ca_df,'device',supported_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51db6723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 outlier found!!!\n"
     ]
    }
   ],
   "source": [
    "# max char length of the search_term column\n",
    "validate_max_length_of_column(ca_df, 'search_term',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03060afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:===================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dates date format matches!!! Null values are skipped in that check, as that check done in previous steps!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# date column format, Null values skipped as that is checked in previous steps\n",
    "validate_date_format_for_column(ca_df, 'date', date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e800309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 outlier found!!!\n"
     ]
    }
   ],
   "source": [
    "# max char length of the domain column\n",
    "validate_max_length_of_column(ca_df, 'domain',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcbd30a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:===================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 outlier found!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sponsored_appearances >= 0\n",
    "validate_min_value_of_column(ca_df,'sponsored_appearances',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a35a41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:===================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 outlier found!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# natural_appearances >= 0\n",
    "validate_min_value_of_column(ca_df,'natural_appearances',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dc36dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:===================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 outlier found!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# pla_appearances >= 0\n",
    "validate_min_value_of_column(ca_df,'pla_appearances',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c8247b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:=====================================================>  (22 + 1) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 outlier found!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ctr >= 0\n",
    "validate_min_value_of_column(ca_df,'ctr',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3ee9930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:===================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 outlier found!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 90:=====================================================>  (22 + 1) / 23]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ctr <= 1.0\n",
    "validate_max_value_of_column(ca_df,'ctr',1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14dfff74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7143661"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping null values from PK (which is not expected to be in dataset)\n",
    "sa_df_not_null_pk = sa_df.na.drop(subset=['search_term', 'device', 'date'])\n",
    "sa_df_not_null_pk.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "265ee324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 96:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+------------+--------------------+\n",
      "|         search_term|      date| device|scrape_count|                  id|\n",
      "+--------------------+----------+-------+------------+--------------------+\n",
      "|$100 000 personal...|2022-05-13|desktop|           1|$100 000 personal...|\n",
      "|$135 nike men's b...|2022-05-13|desktop|           1|$135 nike men's b...|\n",
      "|$199 michael star...|2022-05-13|desktop|           1|$199 michael star...|\n",
      "+--------------------+----------+-------+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# creating id as a unique key for scrape_appearances\n",
    "sa_df_not_null_pk = sa_df_not_null_pk.select(\"*\", concat(sa_df_not_null_pk.search_term, sa_df_not_null_pk.device, sa_df_not_null_pk.date)\n",
    "                         .alias(\"id\"))\n",
    "sa_df_not_null_pk.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c066bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7143660"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We had 1 pair of dulication in scrape_appearances dataframe, which is not expected to exist, to drop it.\n",
    "sa_df_not_null_pk = sa_df_not_null_pk.dropDuplicates()\n",
    "sa_df_not_null_pk.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09ea9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating same id for competitor appearances to match it with scrape appearances\n",
    "ca_df = ca_df.select(\"*\", concat(ca_df.search_term, ca_df.device, ca_df.date)\n",
    "                   .alias(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdeded6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create view and use sql to check weather each row has a corresponding in scrape_appearances dataset \n",
    "sa_df_not_null_pk.createOrReplaceTempView('scrape_appearances')\n",
    "ca_df.createOrReplaceTempView('competitor_appearances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a6cd8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:====================================================>  (22 + 1) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|no_corresponding|\n",
      "+----------------+\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# count where there is no corresponding row\n",
    "query = \"select count(*) as no_corresponding from competitor_appearances where id not in (select id from scrape_appearances)\"\n",
    "result = get_spark().sql(query)\n",
    "result.show()\n",
    "# assert result.rdd.collect()[0][0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7afc9f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 114:===================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+------------------------------+----------+-------+---------+\n",
      "|id                                             |search_term                   |date      |device |domain   |\n",
      "+-----------------------------------------------+------------------------------+----------+-------+---------+\n",
      "|no entry in scrape appearancesdesktop2022-05-13|no entry in scrape appearances|2022-05-13|desktop|click.com|\n",
      "+-----------------------------------------------+------------------------------+----------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The record in competitor_appearances where there is no corresponding in scrape_appearances\n",
    "query = \"select id,search_term,date,device,domain from competitor_appearances where id not in (select id from scrape_appearances)\"\n",
    "result = get_spark().sql(query)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "997e04a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:===================================================>   (47 + 3) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|outlier_sponsored_appearances|\n",
      "+-----------------------------+\n",
      "|                       306589|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count outlier sponsored_appearances records, where sponsored_appearances > scrape_count for corresponding records\n",
    "query = \"select count(*) as outlier_sponsored_appearances from (select ca.id, sponsored_appearances, scrape_count from competitor_appearances ca inner join scrape_appearances sa on ca.id = sa.id) as jn where jn.sponsored_appearances > jn.scrape_count\"\n",
    "result = get_spark().sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "205da0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+------------+\n",
      "|                  id|sponsored_appearances|scrape_count|\n",
      "+--------------------+---------------------+------------+\n",
      "|0 balance credit ...|                   11|           8|\n",
      "|0 balance credit ...|                    9|           8|\n",
      "|0 balance credit ...|                   11|           8|\n",
      "|0 balance credit ...|                   11|           8|\n",
      "|0 balance transfe...|                   16|           8|\n",
      "|0 balance transfe...|                   11|           8|\n",
      "|0 balance transfe...|                   15|           8|\n",
      "|0 transfer credit...|                   12|           8|\n",
      "|0 transfer credit...|                   18|           8|\n",
      "|0 transfer credit...|                   17|           8|\n",
      "|0 transfer credit...|                   14|           8|\n",
      "|0 transfer credit...|                   19|           8|\n",
      "|0 transfer credit...|                   13|           8|\n",
      "|0 transfer credit...|                   10|           8|\n",
      "|1 foot extension ...|                    9|           8|\n",
      "|10 year cddesktop...|                   13|           8|\n",
      "|10 year cddesktop...|                    9|           8|\n",
      "|100 cash out refi...|                   10|           8|\n",
      "|100 cash out refi...|                   19|           8|\n",
      "|100 cash out refi...|                    9|           8|\n",
      "+--------------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Some records where sponsored_appearances > scrape_count for corresponding records\n",
    "query = \"select * from (select ca.id, sponsored_appearances, scrape_count from competitor_appearances ca inner join scrape_appearances sa on ca.id = sa.id) as jn where jn.sponsored_appearances > jn.scrape_count\"\n",
    "result = get_spark().sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38148e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491eca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
